# genai-pipeline-project
# ğŸ§  Generative AI Pipeline & Production Deployment

This project implements a complete, end-to-end CI/CD pipeline for deploying a Hugging Face-based Text Generation Model as a production-ready API, leveraging Azure Databricks for model preparation, Docker for containerization, and Jenkins for automated deployment.

## ğŸš€ Project Overview

The core goal is to demonstrate a robust MLOps (Machine Learning Operations) flow for Generative AI inference.

| Component | Role | Technology |
| :--- | :--- | :--- |
| **Model Prep** | Model training (simulated fine-tuning) and artifact storage. | **Azure Databricks** |
| **API Serving** | Lightweight inference server to handle requests. | **Python/Flask** |
| **Containerization** | Packaging the API and model for consistent deployment. | **Docker** |
| **API Gateway** | Reverse proxy, load balancing, and external exposure. | **Nginx** |
| **CI/CD** | Automated build, test, and deployment upon code push. | **Jenkins/GitHub** |

## ğŸ“ Project Structure

genai-pipeline-project/ â”œâ”€â”€ databricks/ â”‚ â””â”€â”€ notebook.py # Databricks code for training/model prep â”œâ”€â”€ api/ â”‚ â”œâ”€â”€ app.py # Flask/Python application for inference â”‚ â”œâ”€â”€ Dockerfile # Docker image definition for the API â”‚ â”œâ”€â”€ requirements.txt # Python dependencies â”‚ â””â”€â”€ model/ â”‚ â””â”€â”€ gpt2/ # Hugging Face Model Artifacts â”œâ”€â”€ config/ â”‚ â””â”€â”€ nginx.conf # Nginx configuration for reverse proxy â””â”€â”€ ci-cd/ â””â”€â”€ Jenkinsfile # Jenkins pipeline definition â””â”€â”€ README.md